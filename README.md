In this project, we're conducting a comprehensive analysis of machine learning methods (including GLM, XGBoost, and Deep Learning) across datasets of varying sizes (ranging from 100 to 100000). Our objective is to evaluate the performance of these methods in terms of predictive accuracy and computational efficiency. By examining key metrics such as Training RMSE, Holdout RMSE, and Time Taken, we aim to identify the optimal modeling approach and configuration for predictive tasks across a spectrum of dataset scales. Notably, our findings highlight XGBoost Configuration 4 as consistently outperforming others, striking a balance between predictive accuracy and computational efficiency across datasets of different magnitudes. This research provides valuable insights into selecting the most suitable modeling strategy based on dataset size, ensuring effective and efficient predictive modeling in various real-world applications.






